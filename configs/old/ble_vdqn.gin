import balloon_learning_environment.agents.networks
import dopamine.jax.agents.dqn.dqn_agent
import dopamine.replay_memory.circular_replay_buffer
import balloon_learning_environment.agents.marco_polo_exploration

MarcoPoloExploration.exploratory_episode_probability = 0.8
MarcoPoloExploration.exploratory_agent_constructor = @random_walk_agent.RandomWalkAgent
networks.MLPNetwork.num_layers = 8
networks.MLPNetwork.hidden_units = 600
JaxDQNAgent.gamma = 0.993
JaxDQNAgent.update_horizon = 5
JaxDQNAgent.min_replay_history = 500
JaxDQNAgent.update_period = 4
JaxDQNAgent.target_update_period = 100
JaxDQNAgent.epsilon_fn = @dqn_agent.identity_epsilon
JaxDQNAgent.epsilon_train = 0.01
JaxDQNAgent.epsilon_eval = 0.0
JaxDQNAgent.optimizer = 'adam'
JaxDQNAgent.loss_type = 'mse'  # MSE works better with Adam.
JaxDQNAgent.summary_writing_frequency = 1
JaxDQNAgent.allow_partial_reload = True
dqn_agent.create_optimizer.learning_rate = 2e-6
dqn_agent.create_optimizer.eps = 0.00002

OutOfGraphReplayBuffer.replay_capacity = 2000000
OutOfGraphReplayBuffer.batch_size = 32